{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enegori6160/rags/blob/main/16kmodel_summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VPK6iaeLnA4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain==0.0.205\n",
        "!pip install tiktoken==0.3.3\n",
        "!pip install openai==0.27.2\n",
        "!pip install pypdf==3.10.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "import tiktoken\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "# APIKEY取得\n",
        "APIKEY = getpass.getpass(\"APIKEY: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = APIKEY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQ_Pxp8zL_Kw",
        "outputId": "cdd01390-cf41-46ba-acc6-61622caee249"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "APIKEY: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XrDfdYm7MN4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model=\"gpt-3.5-turbo-16k\",\n",
        "    streaming=True,\n",
        "    callbacks=[StreamingStdOutCallbackHandler()],\n",
        ")"
      ],
      "metadata": {
        "id": "Uw00ACQ0Ma_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "human_template = \"\"\"\n",
        "    下記の#で区切られた文章を英語に翻訳してください\n",
        "\n",
        "    ####\n",
        "    {text}\n",
        "    ####\n",
        "    \"\"\"\n",
        "\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\n",
        "prompt = chat_prompt.format_prompt(text=\"こんにちは\").to_messages()"
      ],
      "metadata": {
        "id": "MHhxyndrMbD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WapWuCpcNJlG",
        "outputId": "10f6c3bc-1681-4d07-9bad-3f45c32a9e68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lvOZk6d2MdHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenの数を数えることができる\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "BH8CxHMUMdLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding.encode(\"ねこかいたい\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8C94p2vPz4B",
        "outputId": "e4e748d5-f78b-467c-de2e-2d47148d064d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2243, 255, 22957, 32149, 16995, 28713, 16995]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HS8g_rLcMiNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "human_template = \"\"\"\n",
        "    下記の#で区切られた単語を反対から出力してください\n",
        "\n",
        "    ####\n",
        "    {text}\n",
        "    ####\n",
        "    \"\"\"\n",
        "\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\n",
        "prompt = chat_prompt.format_prompt(text=\"ねこかいたい\").to_messages()"
      ],
      "metadata": {
        "id": "P5jFuBDeP8AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7NyknsfPqDv",
        "outputId": "90d2e3f7-4e98-4824-959e-9d9688e04be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "いたいかこね"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "23wJJ7ZWMjgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ivd0EzuvMjkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IwKOYFZmMjny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 要約処理の実装"
      ],
      "metadata": {
        "id": "6dA1XhVIMjqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_token(text):\n",
        "    print(f\"Total token: {len(encoding.encode(text, disallowed_special=()))}\")\n",
        "\n",
        "\n",
        "def preprocess_text(loader):\n",
        "    docs = loader.load_and_split()\n",
        "    cnt_token = 0\n",
        "    content = \"\"\n",
        "    for i in range(len(docs)):\n",
        "        page_content = docs[i].page_content\n",
        "        cnt_token += len(encoding.encode(page_content, disallowed_special=()))\n",
        "        if cnt_token > 13_000:\n",
        "            break\n",
        "        content += page_content\n",
        "    count_token(content)\n",
        "    return content"
      ],
      "metadata": {
        "id": "dSdvJQtbMnKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarizer(content, use_system_message=True):\n",
        "    template = \"あなたはAI機械学習が専門の研究者です。出力は全てマークダウン形式で、各セクションとそれに紐づく内容を日本語で出力します。\"\n",
        "    system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "    human_template = \"\"\"\n",
        "    論文: {text}\n",
        "\n",
        "    上記の論文において、次のステップでタスクを行ってください\n",
        "    1. 各セクションのタイトルを抽出する\n",
        "    2. そのタイトルの要約を明確に意味がわかるように行う。必要に応じて専門知識で補完する。\n",
        "\n",
        "    \"\"\"\n",
        "    if not use_system_message:\n",
        "        human_template = template + human_template\n",
        "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "    if not use_system_message:\n",
        "        chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\n",
        "    else:\n",
        "        chat_prompt = ChatPromptTemplate.from_messages(\n",
        "            [system_message_prompt, human_message_prompt]\n",
        "        )\n",
        "    prompt = chat_prompt.format_prompt(text=content).to_messages()\n",
        "    llm = ChatOpenAI(\n",
        "        temperature=0,\n",
        "        model=\"gpt-3.5-turbo-16k\",\n",
        "        streaming=True,\n",
        "        callbacks=[StreamingStdOutCallbackHandler()],\n",
        "    )\n",
        "    return llm(prompt)"
      ],
      "metadata": {
        "id": "tA04FDp_MnRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c5Gwa5Pw3vxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"lora.pdf\")"
      ],
      "metadata": {
        "id": "BVaDVUS7Mpij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content = preprocess_text(loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTqCJsovMpmx",
        "outputId": "bb9b3b12-ea15-42bd-8d69-5169e4c0b6e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total token: 12131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "pHK5M1WiTcyY",
        "outputId": "2b73f439-6088-4458-d51f-1ae19552b20f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LORA: L OW-RANK ADAPTATION OF LARGE LAN-\\nGUAGE MODELS\\nEdward Hu\\x03Yelong Shen\\x03Phillip Wallis Zeyuan Allen-Zhu\\nYuanzhi Li Shean Wang Lu Wang Weizhu Chen\\nMicrosoft Corporation\\nfedwardhu, yeshe, phwallis, zeyuana,\\nyuanzhil, swang, luw, wzchen g@microsoft.com\\nyuanzhil@andrew.cmu.edu\\n(Version 2)\\nABSTRACT\\nAn important paradigm of natural language processing consists of large-scale pre-\\ntraining on general domain data and adaptation to particular tasks or domains. As\\nwe pre-train larger models, full ﬁne-tuning, which retrains all model parameters,\\nbecomes less feasible. Using GPT-3 175B as an example – deploying indepen-\\ndent instances of ﬁne-tuned models, each with 175B parameters, is prohibitively\\nexpensive. We propose Low-RankAdaptation, or LoRA, which freezes the pre-\\ntrained model weights and injects trainable rank decomposition matrices into each\\nlayer of the Transformer architecture, greatly reducing the number of trainable pa-\\nrameters for downstream tasks. Compared to GPT-3 175B ﬁne-tuned with Adam,\\nLoRA can reduce the number of trainable parameters by 10,000 times and the\\nGPU memory requirement by 3 times. LoRA performs on-par or better than ﬁne-\\ntuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite hav-\\ning fewer trainable parameters, a higher training throughput, and, unlike adapters,\\nno additional inference latency . We also provide an empirical investigation into\\nrank-deﬁciency in language model adaptation, which sheds light on the efﬁcacy of\\nLoRA. We release a package that facilitates the integration of LoRA with PyTorch\\nmodels and provide our implementations and model checkpoints for RoBERTa,\\nDeBERTa, and GPT-2 at https://github.com/microsoft/LoRA .\\n1 I NTRODUCTION\\nPretrained \\nWeights\\n𝑊∈ℝ𝑑×𝑑\\nxh\\n𝐵=0\\n𝐴=𝒩(0,𝜎2)\\n𝑑𝑟Pretrained \\nWeights\\n𝑊∈ℝ𝑑×𝑑\\nxf(x)\\n𝑑\\nFigure 1: Our reparametriza-\\ntion. We only train AandB.Many applications in natural language processing rely on adapt-\\ningonelarge-scale, pre-trained language model to multiple down-\\nstream applications. Such adaptation is usually done via ﬁne-tuning ,\\nwhich updates all the parameters of the pre-trained model. The ma-\\njor downside of ﬁne-tuning is that the new model contains as many\\nparameters as in the original model. As larger models are trained\\nevery few months, this changes from a mere “inconvenience” for\\nGPT-2 (Radford et al., b) or RoBERTa large (Liu et al., 2019) to a\\ncritical deployment challenge for GPT-3 (Brown et al., 2020) with\\n175 billion trainable parameters.1\\nMany sought to mitigate this by adapting only some parameters or\\nlearning external modules for new tasks. This way, we only need\\nto store and load a small number of task-speciﬁc parameters in ad-\\ndition to the pre-trained model for each task, greatly boosting the\\noperational efﬁciency when deployed. However, existing techniques\\n\\x03Equal contribution.\\n0Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency.\\n1While GPT-3 175B achieves non-trivial performance with few-shot learning, ﬁne-tuning boosts its perfor-\\nmance signiﬁcantly as shown in Appendix A.\\n1arXiv:2106.09685v2  [cs.CL]  16 Oct 2021often introduce inference latency (Houlsby et al., 2019; Rebufﬁ et al., 2017) by extending model\\ndepth or reduce the model’s usable sequence length (Li & Liang, 2021; Lester et al., 2021; Ham-\\nbardzumyan et al., 2020; Liu et al., 2021) (Section 3). More importantly, these method often fail to\\nmatch the ﬁne-tuning baselines, posing a trade-off between efﬁciency and model quality.\\nWe take inspiration from Li et al. (2018a); Aghajanyan et al. (2020) which show that the learned\\nover-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the\\nchange in weights during model adaptation also has a low “intrinsic rank”, leading to our proposed\\nLow-RankAdaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural\\nnetwork indirectly by optimizing rank decomposition matrices of the dense layers’ change during\\nadaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3\\n175B as an example, we show that a very low rank (i.e., rin Figure 1 can be one or two) sufﬁces even\\nwhen the full rank (i.e., d) is as high as 12,288, making LoRA both storage- and compute-efﬁcient.\\nLoRA possesses several key advantages.\\n• A pre-trained model can be shared and used to build many small LoRA modules for dif-\\nferent tasks. We can freeze the shared model and efﬁciently switch tasks by replacing the\\nmatricesAandBin Figure 1, reducing the storage requirement and task-switching over-\\nhead signiﬁcantly.\\n• LoRA makes training more efﬁcient and lowers the hardware barrier to entry by up to 3\\ntimes when using adaptive optimizers since we do not need to calculate the gradients or\\nmaintain the optimizer states for most parameters. Instead, we only optimize the injected,\\nmuch smaller low-rank matrices.\\n• Our simple linear design allows us to merge the trainable matrices with the frozen weights\\nwhen deployed, introducing no inference latency compared to a fully ﬁne-tuned model, by\\nconstruction.\\n• LoRA is orthogonal to many prior methods and can be combined with many of them, such\\nas preﬁx-tuning. We provide an example in Appendix E.\\nTerminologies and Conventions We make frequent references to the Transformer architecture\\nand use the conventional terminologies for its dimensions. We call the input and output di-\\nmension size of a Transformer layer dmodel . We useWq,Wk,Wv, andWoto refer to the\\nquery/key/value/output projection matrices in the self-attention module. WorW0refers to a pre-\\ntrained weight matrix and \\x01Wits accumulated gradient update during adaptation. We use rto\\ndenote the rank of a LoRA module. We follow the conventions set out by (Vaswani et al., 2017;\\nBrown et al., 2020) and use Adam (Loshchilov & Hutter, 2019; Kingma & Ba, 2017) for model\\noptimization and use a Transformer MLP feedforward dimension dffn= 4\\x02dmodel .\\n2 P ROBLEM STATEMENT\\nWhile our proposal is agnostic to training objective, we focus on language modeling as our motivat-\\ning use case. Below is a brief description of the language modeling problem and, in particular, the\\nmaximization of conditional probabilities given a task-speciﬁc prompt.\\nSuppose we are given a pre-trained autoregressive language model P\\x08(yjx)parametrized by \\x08.\\nFor instance, P\\x08(yjx)can be a generic multi-task learner such as GPT (Radford et al., b; Brown\\net al., 2020) based on the Transformer architecture (Vaswani et al., 2017). Consider adapting this\\npre-trained model to downstream conditional text generation tasks, such as summarization, machine\\nreading comprehension (MRC), and natural language to SQL (NL2SQL). Each downstream task is\\nrepresented by a training dataset of context-target pairs: Z=f(xi;yi)gi=1;::;N, where both xiand\\nyiare sequences of tokens. For example, in NL2SQL, xiis a natural language query and yiits\\ncorresponding SQL command; for summarization, xiis the content of an article and yiits summary.\\n2During full ﬁne-tuning, the model is initialized to pre-trained weights \\x080and updated to \\x080+ \\x01\\x08\\nby repeatedly following the gradient to maximize the conditional language modeling objective:\\nmax\\n\\x08X\\n(x;y)2ZjyjX\\nt=1log(P\\x08(ytjx;y<t)) (1)\\nOne of the main drawbacks for full ﬁne-tuning is that for each downstream task, we learn a different\\nset of parameters \\x01\\x08whose dimensionj\\x01\\x08jequalsj\\x080j. Thus, if the pre-trained model is large\\n(such as GPT-3 with j\\x080j\\x19175Billion), storing and deploying many independent instances of\\nﬁne-tuned models can be challenging, if at all feasible.\\nIn this paper, we adopt a more parameter-efﬁcient approach, where the task-speciﬁc parameter\\nincrement \\x01\\x08 = \\x01\\x08(\\x02) is further encoded by a much smaller-sized set of parameters \\x02with\\nj\\x02j\\x1cj \\x080j. The task of ﬁnding \\x01\\x08thus becomes optimizing over \\x02:\\nmax\\n\\x02X\\n(x;y)2ZjyjX\\nt=1log\\x00\\np\\x080+\\x01\\x08(\\x02) (ytjx;y<t)\\x01\\n(2)\\nIn the subsequent sections, we propose to use a low-rank representation to encode \\x01\\x08that is both\\ncompute- and memory-efﬁcient. When the pre-trained model is GPT-3 175B, the number of train-\\nable parametersj\\x02jcan be as small as 0:01% ofj\\x080j.\\n3 A REN’TEXISTING SOLUTIONS GOOD ENOUGH ?\\nThe problem we set out to tackle is by no means new. Since the inception of transfer learning, dozens\\nof works have sought to make model adaptation more parameter- and compute-efﬁcient. See Sec-\\ntion 6 for a survey of some of the well-known works. Using language modeling as an example, there\\nare two prominent strategies when it comes to efﬁcient adaptations: adding adapter layers (Houlsby\\net al., 2019; Rebufﬁ et al., 2017; Pfeiffer et al., 2021; R ¨uckl´e et al., 2020) or optimizing some forms\\nof the input layer activations (Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020;\\nLiu et al., 2021). However, both strategies have their limitations, especially in a large-scale and\\nlatency-sensitive production scenario.\\nAdapter Layers Introduce Inference Latency There are many variants of adapters. We focus\\non the original design by Houlsby et al. (2019) which has two adapter layers per Transformer block\\nand a more recent one by Lin et al. (2020) which has only one per block but with an additional\\nLayerNorm (Ba et al., 2016). While one can reduce the overall latency by pruning layers or exploit-\\ning multi-task settings (R ¨uckl´e et al., 2020; Pfeiffer et al., 2021), there is no direct ways to bypass\\nthe extra compute in adapter layers. This seems like a non-issue since adapter layers are designed\\nto have few parameters (sometimes <1% of the original model) by having a small bottleneck di-\\nmension, which limits the FLOPs they can add. However, large neural networks rely on hardware\\nparallelism to keep the latency low, and adapter layers have to be processed sequentially. This makes\\na difference in the online inference setting where the batch size is typically as small as one. In a\\ngeneric scenario without model parallelism, such as running inference on GPT-2 (Radford et al., b)\\nmedium on a single GPU, we see a noticeable increase in latency when using adapters, even with a\\nvery small bottleneck dimension (Table 1).\\nThis problem gets worse when we need to shard the model as done in Shoeybi et al. (2020); Lep-\\nikhin et al. (2020), because the additional depth requires more synchronous GPU operations such as\\nAllReduce andBroadcast , unless we store the adapter parameters redundantly many times.\\nDirectly Optimizing the Prompt is Hard The other direction, as exempliﬁed by preﬁx tuning (Li\\n& Liang, 2021), faces a different challenge. We observe that preﬁx tuning is difﬁcult to optimize\\nand that its performance changes non-monotonically in trainable parameters, conﬁrming similar\\nobservations in the original paper. More fundamentally, reserving a part of the sequence length for\\nadaptation necessarily reduces the sequence length available to process a downstream task, which\\nwe suspect makes tuning the prompt less performant compared to other methods. We defer the studyadaptation necessarily reduces the sequence length available to process a downstream task, which\\nwe suspect makes tuning the prompt less performant compared to other methods. We defer the study\\non task performance to Section 5.\\n3Batch Size 32 16 1\\nSequence Length 512 256 128\\nj\\x02j 0.5M 11M 11M\\nFine-Tune/LoRA 1449.4\\x060.8 338.0 \\x060.6 19.8 \\x062.7\\nAdapterL1482.0\\x061.0 (+2.2%) 354.8 \\x060.5 (+5.0%) 23.9 \\x062.1 (+20.7%)\\nAdapterH1492.2\\x061.0 (+3.0%) 366.3 \\x060.5 (+8.4%) 25.8 \\x062.2 (+30.3%)\\nTable 1: Infernece latency of a single forward pass in GPT-2 medium measured in milliseconds, av-\\neraged over 100 trials. We use an NVIDIA Quadro RTX8000. “ j\\x02j” denotes the number of trainable\\nparameters in adapter layers. AdapterLand AdapterHare two variants of adapter tuning, which we\\ndescribe in Section 5.1. The inference latency introduced by adapter layers can be signiﬁcant in an\\nonline, short-sequence-length scenario. See the full study in Appendix B.\\n4 O URMETHOD\\nWe describe the simple design of LoRA and its practical beneﬁts. The principles outlined here apply\\nto any dense layers in deep learning models, though we only focus on certain weights in Transformer\\nlanguage models in our experiments as the motivating use case.\\n4.1 L OW-RANK -PARAMETRIZED UPDATE MATRICES\\nA neural network contains many dense layers which perform matrix multiplication. The weight\\nmatrices in these layers typically have full-rank. When adapting to a speciﬁc task, Aghajanyan et al.\\n(2020) shows that the pre-trained language models have a low “instrisic dimension” and can still\\nlearn efﬁciently despite a random projection to a smaller subspace. Inspired by this, we hypothe-\\nsize the updates to the weights also have a low “intrinsic rank” during adaptation. For a pre-trained\\nweight matrix W02Rd\\x02k, we constrain its update by representing the latter with a low-rank de-\\ncomposition W0+ \\x01W=W0+BA, whereB2Rd\\x02r;A2Rr\\x02k, and the rank r\\x1cmin(d;k).\\nDuring training, W0is frozen and does not receive gradient updates, while AandBcontain trainable\\nparameters. Note both W0and\\x01W=BAare multiplied with the same input, and their respective\\noutput vectors are summed coordinate-wise. For h=W0x, our modiﬁed forward pass yields:\\nh=W0x+ \\x01Wx=W0x+BAx (3)\\nWe illustrate our reparametrization in Figure 1. We use a random Gaussian initialization for Aand\\nzero forB, so\\x01W=BAis zero at the beginning of training. We then scale \\x01Wx by\\x0b\\nr, where\\x0b\\nis a constant in r. When optimizing with Adam, tuning \\x0bis roughly the same as tuning the learning\\nrate if we scale the initialization appropriately. As a result, we simply set \\x0bto the ﬁrstrwe try\\nand do not tune it. This scaling helps to reduce the need to retune hyperparameters when we vary\\nr(Yang & Hu, 2021).\\nA Generalization of Full Fine-tuning. A more general form of ﬁne-tuning allows the training of\\na subset of the pre-trained parameters. LoRA takes a step further and does not require the accumu-\\nlated gradient update to weight matrices to have full-rank during adaptation. This means that when\\napplying LoRA to all weight matrices and training all biases2, we roughly recover the expressive-\\nness of full ﬁne-tuning by setting the LoRA rank rto the rank of the pre-trained weight matrices. In\\nother words, as we increase the number of trainable parameters3, training LoRA roughly converges\\nto training the original model, while adapter-based methods converges to an MLP and preﬁx-based\\nmethods to a model that cannot take long input sequences.\\nNo Additional Inference Latency. When deployed in production, we can explicitly compute and\\nstoreW=W0+BA and perform inference as usual. Note that both W0andBA are inRd\\x02k.\\nWhen we need to switch to another downstream task, we can recover W0by subtracting BAand\\nthen adding a different B0A0, a quick operation with very little memory overhead. Critically, this\\n2They represent a negligible number of parameters compared to weights.\\n3An inevitability when adapting to hard tasks.\\n4guarantees that we do not introduce any additional latency during inference compared to a ﬁne-tuned\\nmodel by construction.\\n4.2 A PPLYING LORA TOTRANSFORMER\\nIn principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the\\nnumber of trainable parameters. In the Transformer architecture, there are four weight matrices in\\nthe self-attention module ( Wq;Wk;Wv;Wo) and two in the MLP module. We treat Wq(orWk,Wv)\\nas a single matrix of dimension dmodel\\x02dmodel , even though the output dimension is usually sliced\\ninto attention heads. We limit our study to only adapting the attention weights for downstream\\ntasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity\\nand parameter-efﬁciency.We further study the effect on adapting different types of attention weight\\nmatrices in a Transformer in Section 7.1. We leave the empirical investigation of adapting the MLP\\nlayers, LayerNorm layers, and biases to a future work.\\nPractical Beneﬁts and Limitations. The most signiﬁcant beneﬁt comes from the reduction in\\nmemory and storage usage. For a large Transformer trained with Adam, we reduce that VRAM\\nusage by up to 2=3ifr\\x1cdmodel as we do not need to store the optimizer states for the frozen\\nparameters. On GPT-3 175B, we reduce the VRAM consumption during training from 1.2TB to\\n350GB. With r= 4and only the query and value projection matrices being adapted, the checkpoint\\nsize is reduced by roughly 10,000 \\x02(from 350GB to 35MB)4. This allows us to train with signiﬁ-\\ncantly fewer GPUs and avoid I/O bottlenecks. Another beneﬁt is that we can switch between tasks\\nwhile deployed at a much lower cost by only swapping the LoRA weights as opposed to all the\\nparameters. This allows for the creation of many customized models that can be swapped in and out\\non the ﬂy on machines that store the pre-trained weights in VRAM. We also observe a 25% speedup\\nduring training on GPT-3 175B compared to full ﬁne-tuning5as we do not need to calculate the\\ngradient for the vast majority of the parameters.\\nLoRA also has its limitations. For example, it is not straightforward to batch inputs to different tasks\\nwith different AandBin a single forward pass, if one chooses to absorb AandBintoWto eliminate\\nadditional inference latency. Though it is possible to not merge the weights and dynamically choose\\nthe LoRA modules to use for samples in a batch for scenarios where latency is not critical.\\n5 E MPIRICAL EXPERIMENTS\\nWe evaluate the downstream task performance of LoRA on RoBERTa (Liu et al., 2019), De-\\nBERTa (He et al., 2021), and GPT-2 (Radford et al., b), before scaling up to GPT-3 175B (Brown\\net al., 2020). Our experiments cover a wide range of tasks, from natural language understanding\\n(NLU) to generation (NLG). Speciﬁcally, we evaluate on the GLUE (Wang et al., 2019) benchmark\\nfor RoBERTa and DeBERTa. We follow the setup of Li & Liang (2021) on GPT-2 for a direct com-\\nparison and add WikiSQL (Zhong et al., 2017) (NL to SQL queries) and SAMSum (Gliwa et al.,\\n2019) (conversation summarization) for large-scale experiments on GPT-3. See Appendix C for\\nmore details on the datasets we use. We use NVIDIA Tesla V100 for all experiments.\\n5.1 B ASELINES\\nTo compare with other baselines broadly, we replicate the setups used by prior work and reuse their\\nreported numbers whenever possible. This, however, means that some baselines might only appear\\nin certain experiments.\\nFine-Tuning (FT) is a common approach for adaptation. During ﬁne-tuning, the model is initialized\\nto the pre-trained weights and biases, and all model parameters undergo gradient updates.A simple\\nvariant is to update only some layers while freezing others. We include one such baseline reported\\nin prior work (Li & Liang, 2021) on GPT-2, which adapts just the last two layers ( FTTop2).\\n4We still need the 350GB model during deployment; however, storing 100 adapted models only requires\\n350GB + 35MB * 100 \\x19354GB as opposed to 100 * 350GB \\x1935TB.4We still need the 350GB model during deployment; however, storing 100 adapted models only requires\\n350GB + 35MB * 100 \\x19354GB as opposed to 100 * 350GB \\x1935TB.\\n5For GPT-3 175B, the training throughput for full ﬁne-tuning is 32.5 tokens/s per V100 GPU; with the same\\nnumber of weight shards for model parallelism, the throughput is 43.1 tokens/s per V100 GPU for LoRA.\\n5Model & Method # Trainable\\nParameters MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg.\\nRoB base(FT)* 125.0M 87.6 94.8 90.2 63.6 92.8 91.9 78.7 91.2 86.4\\nRoB base(BitFit)* 0.1M 84.7 93.7 92.7 62.0 91.8 84.0 81.5 90.8 85.2\\nRoB base(AdptD)* 0.3M 87.1\\x06.094.2\\x06.188.5\\x061.160.8\\x06.493.1\\x06.190.2\\x06.071.5\\x062.789.7\\x06.384.4\\nRoB base(AdptD)* 0.9M 87.3\\x06.194.7\\x06.388.4\\x06.162.6\\x06.993.0\\x06.290.6\\x06.075.9\\x062.290.3\\x06.185.4\\nRoB base(LoRA) 0.3M 87.5\\x06.395.1\\x06.289.7\\x06.763.4\\x061.293.3\\x06.390.8\\x06.186.6\\x06.791.5\\x06.287.2\\nRoB large(FT)* 355.0M 90.2 96.4 90.9 68.0 94.7 92.2 86.6 92.4 88.9\\nRoB large(LoRA) 0.8M 90.6\\x06.296.2\\x06.590.9\\x061.268.2\\x061.994.9\\x06.391.6\\x06.187.4\\x062.592.6\\x06.289.0\\nRoB large(AdptP)y 3.0M 90.2\\x06.396.1\\x06.390.2\\x06.768.3\\x061.094.8\\x06.291.9\\x06.183.8\\x062.992.1\\x06.788.4\\nRoB large(AdptP)y 0.8M 90.5\\x06.396.6\\x06.289.7\\x061.267.8\\x062.594.8\\x06.391.7\\x06.280.1\\x062.991.9\\x06.487.9\\nRoB large(AdptH)y 6.0M 89.9\\x06.596.2\\x06.388.7\\x062.966.5\\x064.494.7\\x06.292.1\\x06.183.4\\x061.191.0\\x061.787.8\\nRoB large(AdptH)y 0.8M 90.3\\x06.396.3\\x06.587.7\\x061.766.3\\x062.094.7\\x06.291.5\\x06.172.9\\x062.991.5\\x06.586.4\\nRoB large(LoRA)y 0.8M 90.6\\x06.296.2\\x06.590.2\\x061.068.2\\x061.994.8\\x06.391.6\\x06.285.2\\x061.192.3\\x06.588.6\\nDeB XXL(FT)* 1500.0M 91.8 97.2 92.0 72.0 96.0 92.7 93.9 92.9 91.1\\nDeB XXL(LoRA) 4.7M 91.9\\x06.296.9\\x06.292.6\\x06.672.4\\x061.196.0\\x06.192.9\\x06.194.9\\x06.493.0\\x06.291.3\\nTable 2: RoBERTa base, RoBERTa large, and DeBERTa XXLwith different adaptation methods on the\\nGLUE benchmark. We report the overall (matched and mismatched) accuracy for MNLI, Matthew’s\\ncorrelation for CoLA, Pearson correlation for STS-B, and accuracy for other tasks. Higher is better\\nfor all metrics. * indicates numbers published in prior works. yindicates runs conﬁgured in a setup\\nsimilar to Houlsby et al. (2019) for a fair comparison.\\nBias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else.\\nContemporarily, this baseline has also been studied by BitFit (Zaken et al., 2021).\\nPreﬁx-embedding tuning (PreEmbed) inserts special tokens among the input tokens. These spe-\\ncial tokens have trainable word embeddings and are generally not in the model’s vocabulary. Where\\nto place such tokens can have an impact on performance. We focus on “preﬁxing”, which prepends\\nsuch tokens to the prompt, and “inﬁxing”, which appends to the prompt; both are discussed in Li &\\nLiang (2021). We use lp(resp.li) denote the number of preﬁx (resp. inﬁx) tokens. The number of\\ntrainable parameters is j\\x02j=dmodel\\x02(lp+li).\\nPreﬁx-layer tuning (PreLayer) is an extension to preﬁx-embedding tuning. Instead of just learning\\nthe word embeddings (or equivalently, the activations after the embedding layer) for some special\\ntokens, we learn the activations after every Transformer layer. The activations computed from pre-\\nvious layers are simply replaced by trainable ones. The resulting number of trainable parameters is\\nj\\x02j=L\\x02dmodel\\x02(lp+li), whereLis the number of Transformer layers.\\nAdapter tuning as proposed in Houlsby et al. (2019) inserts adapter layers between the self-\\nattention module (and the MLP module) and the subsequent residual connection. There are two\\nfully connected layers with biases in an adapter layer with a nonlinearity in between. We call this\\noriginal design AdapterH. Recently, Lin et al. (2020) proposed a more efﬁcient design with the\\nadapter layer applied only after the MLP module and after a LayerNorm. We call it AdapterL. This\\nis very similar to another deign proposed in Pfeiffer et al. (2021), which we call AdapterP. We also\\ninclude another baseline call AdapterDrop (R ¨uckl´e et al., 2020) which drops some adapter layers for\\ngreater efﬁciency ( AdapterD). We cite numbers from prior works whenever possible to maximize\\nthe number of baselines we compare with; they are in rows with an asterisk (*) in the ﬁrst column.\\nIn all cases, we have j\\x02j=^LAdpt\\x02(2\\x02dmodel\\x02r+r+dmodel )+2\\x02^LLN\\x02dmodel where ^LAdpt\\nis the number of adapter layers and ^LLNthe number of trainable LayerNorms (e.g., in AdapterL).\\nLoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.is the number of adapter layers and ^LLNthe number of trainable LayerNorms (e.g., in AdapterL).\\nLoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.\\nAs mentioned in Section 4.2, we only apply LoRA to WqandWvin most experiments for simplicity.\\nThe number of trainable parameters is determined by the rank rand the shape of the original weights:\\nj\\x02j= 2\\x02^LLoRA\\x02dmodel\\x02r, where ^LLoRA is the number of weight matrices we apply LoRA to.\\n6Model & Method # Trainable E2E NLG Challenge\\nParameters BLEU NIST MET ROUGE-L CIDEr\\nGPT-2 M (FT)* 354.92M 68.2 8.62 46.2 71.0 2.47\\nGPT-2 M (AdapterL)* 0.37M 66.3 8.41 45.0 69.8 2.40\\nGPT-2 M (AdapterL)* 11.09M 68.9 8.71 46.1 71.3 2.47\\nGPT-2 M (AdapterH) 11.09M 67.3\\x06.68.50\\x06.07 46.0\\x06.2 70.7\\x06.2 2.44\\x06.01\\nGPT-2 M (FTTop2)* 25.19M 68.1 8.59 46.0 70.8 2.41\\nGPT-2 M (PreLayer)* 0.35M 69.7 8.81 46.1 71.4 2.49\\nGPT-2 M (LoRA) 0.35M 70.4\\x06.18.85\\x06.02 46.8\\x06.2 71.8\\x06.1 2.53\\x06.02\\nGPT-2 L (FT)* 774.03M 68.5 8.78 46.0 69.9 2.45\\nGPT-2 L (AdapterL) 0.88M 69.1\\x06.18.68\\x06.03 46.3\\x06.0 71.4\\x06.2 2.49\\x06.0\\nGPT-2 L (AdapterL) 23.00M 68.9\\x06.38.70\\x06.04 46.1\\x06.1 71.3\\x06.2 2.45\\x06.02\\nGPT-2 L (PreLayer)* 0.77M 70.3 8.85 46.2 71.7 2.47\\nGPT-2 L (LoRA) 0.77M 70.4\\x06.18.89\\x06.02 46.8\\x06.2 72.0\\x06.2 2.47\\x06.02\\nTable 3: GPT-2 medium (M) and large (L) with different adaptation methods on the E2E NLG\\nChallenge. For all metrics, higher is better. LoRA outperforms several baselines with comparable\\nor fewer trainable parameters. Conﬁdence intervals are shown for experiments we ran. * indicates\\nnumbers published in prior works.\\n5.2 R OBERT A BASE /LARGE\\nRoBERTa (Liu et al., 2019) optimized the pre-training recipe originally proposed in BERT (Devlin\\net al., 2019a) and boosted the latter’s task performance without introducing many more trainable\\nparameters. While RoBERTa has been overtaken by much larger models on NLP leaderboards\\nsuch as the GLUE benchmark (Wang et al., 2019) in recent years, it remains a competitive and\\npopular pre-trained model for its size among practitioners. We take the pre-trained RoBERTa base\\n(125M) and RoBERTa large (355M) from the HuggingFace Transformers library (Wolf et al., 2020)\\nand evaluate the performance of different efﬁcient adaptation approaches on tasks from the GLUE\\nbenchmark. We also replicate Houlsby et al. (2019) and Pfeiffer et al. (2021) according to their\\nsetup. To ensure a fair comparison, we make two crucial changes to how we evaluate LoRA when\\ncomparing with adapters. First, we use the same batch size for all tasks and use a sequence length\\nof 128 to match the adapter baselines. Second, we initialize the model to the pre-trained model for\\nMRPC, RTE, and STS-B, not a model already adapted to MNLI like the ﬁne-tuning baseline. Runs\\nfollowing this more restricted setup from Houlsby et al. (2019) are labeled with y. The result is\\npresented in Table 2 (Top Three Sections). See Section D.1 for details on the hyperparameters used.\\n5.3 D EBERT AXXL\\nDeBERTa (He et al., 2021) is a more recent variant of BERT that is trained on a much larger\\nscale and performs very competitively on benchmarks such as GLUE (Wang et al., 2019) and Su-\\nperGLUE (Wang et al., 2020). We evaluate if LoRA can still match the performance of a fully\\nﬁne-tuned DeBERTa XXL (1.5B) on GLUE. The result is presented in Table 2 (Bottom Section).\\nSee Section D.2 for details on the hyperparameters used.\\n5.4 GPT-2 MEDIUM /LARGE\\nHaving shown that LoRA can be a competitive alternative to full ﬁne-tuning on NLU, we hope to\\nanswer if LoRA still prevails on NLG models, such as GPT-2 medium and large (Radford et al.,\\nb). We keep our setup as close as possible to Li & Liang (2021) for a direct comparison. Due\\nto space constraint, we only present our result on E2E NLG Challenge (Table 3) in this section.\\nSee Section F.1 for results on WebNLG (Gardent et al., 2017) and DART (Nan et al., 2020). We\\ninclude a list of the hyperparameters used in Section D.3.\\n7Model&Method# Trainable WikiSQL MNLI-m SAMSum\\nParameters Acc. (%) Acc. (%) R1/R2/RL\\nGPT-3 (FT) 175,255.8M 73.8 89.5 52.0/28.0/44.5\\nGPT-3 (BitFit) 14.2M 71.3 91.0 51.3/27.4/43.5\\nGPT-3 (PreEmbed) 3.2M 63.1 88.6 48.3/24.2/40.5\\nGPT-3 (PreLayer) 20.2M 70.1 89.5 50.8/27.3/43.5\\nGPT-3 (AdapterH) 7.1M 71.9 89.8 53.0/28.9/44.8\\nGPT-3 (AdapterH) 40.1M 73.2 91.5 53.2/29.0/45.1\\nGPT-3 (LoRA) 4.7M 73.4 91.7 53.8/29.8/45.9\\nGPT-3 (LoRA) 37.7M 74.0 91.6 53.4/29.2/45.1\\nTable 4: Performance of different adaptation methods on GPT-3 175B. We report the logical form\\nvalidation accuracy on WikiSQL, validation accuracy on MultiNLI-matched, and Rouge-1/2/L on\\nSAMSum. LoRA performs better than prior approaches, including full ﬁne-tuning. The results\\non WikiSQL have a ﬂuctuation around \\x060:5%, MNLI-m around \\x060:1%, and SAMSum around\\n\\x060:2/\\x060:2/\\x060:1for the three metrics.\\n5.5 S CALING UP TO GPT-3 175B\\nAs a ﬁnal stress test for LoRA, we scale up to GPT-3 with 175 billion parameters. Due to the high\\ntraining cost, we only report the typical standard deviation for a given task over random seeds, as\\nopposed to providing one for every entry. See Section D.4 for details on the hyperparameters used.\\nAs shown in Table 4, LoRA matches or exceeds the ﬁne-tuning baseline on all three datasets. Note\\nthat not all methods beneﬁt monotonically from having more trainable parameters, as shown in Fig-\\nure 2. We observe a signiﬁcant performance drop when we use more than 256 special tokens for\\npreﬁx-embedding tuning or more than 32 special tokens for preﬁx-layer tuning. This corroborates\\nsimilar observations in Li & Liang (2021). While a thorough investigation into this phenomenon\\nis out-of-scope for this work, we suspect that having more special tokens causes the input distri-\\nbution to shift further away from the pre-training data distribution. Separately, we investigate the\\nperformance of different adaptation approaches in the low-data regime in Section F.3.\\n6 7 8 9 10 11\\nlog10 # Trainable Parameters0.550.600.650.700.75Validation Accuracy\\nWikiSQL\\nMethod\\nFine-Tune\\nPrefixEmbed\\nPrefixLayer\\nAdapter(H)\\nLoRA\\n6 7 8 9 10 11\\nlog10 # Trainable Parameters0.840.860.880.900.92\\nMultiNLI-matched\\nFigure 2: GPT-3 175B validation accuracy vs. number of trainable parameters of several adaptation\\nmethods on WikiSQL and MNLI-matched. LoRA exhibits better scalability and task performance.\\nSee Section F.2 for more details on the plotted data points.\\n6 R ELATED WORKS\\nTransformer Language Models. Transformer (Vaswani et al., 2017) is a sequence-to-sequence\\narchitecture that makes heavy use of self-attention. Radford et al. (a) applied it to autoregressive lan-\\nguage modeling by using a stack of Transformer decoders. Since then, Transformer-based language\\nmodels have dominated NLP, achieving the state-of-the-art in many tasks. A new paradigm emerged\\nwith BERT (Devlin et al., 2019b) and GPT-2 (Radford et al., b) – both are large Transformer lan-\\n8guage models trained on a large amount of text – where ﬁne-tuning on task-speciﬁc data after pre-\\ntraining on general domain data provides a signiﬁcant performance gain compared to training on\\ntask-speciﬁc data directly. Training larger Transformers generally results in better performance and\\nremains an active research direction. GPT-3 (Brown et al., 2020) is the largest single Transformer\\nlanguage model trained to-date with 175B parameters.\\nPrompt Engineering and Fine-Tuning. While GPT-3 175B can adapt its behavior with just a\\nfew additional training examples, the result depends heavily on the input prompt (Brown et al.,\\n2020). This necessitates an empirical art of composing and formatting the prompt to maximize a\\nmodel’s performance on a desired task, which is known as prompt engineering or prompt hacking.\\nFine-tuning retrains a model pre-trained on general domains to a speciﬁc task Devlin et al. (2019b);\\nRadford et al. (a). Variants of it include learning just a subset of the parameters Devlin et al. (2019b);\\nCollobert & Weston (2008), yet practitioners often retrain all of them to maximize the downstream\\nperformance. However, the enormity of GPT-3 175B makes it challenging to perform ﬁne-tuning in\\nthe usual way due to the large checkpoint it produces and the high hardware barrier to entry since it\\nhas the same memory footprint as pre-training.\\nParameter-Efﬁcient Adaptation. Many have proposed inserting adapter layers between existing\\nlayers in a neural network (Houlsby et al., 2019; Rebufﬁ et al., 2017; Lin et al., 2020). Our method\\nuses a similar bottleneck structure to impose a low-rank constraint on the weight updates. The\\nkey functional difference is that our learned weights can be merged with the main weights during\\ninference, thus not introducing any latency, which is not the case for the adapter layers (Section 3).\\nA comtenporary extension of adapter is COMPACTER (Mahabadi et al., 2021), which essentially\\nparametrizes the adapter layers using Kronecker products with some predetermined weight sharing\\nscheme. Similarly, combining LoRA with other tensor product-based methods could potentially\\nimprove its parameter efﬁciency, which we leave to future work. More recently, many proposed\\noptimizing the input word embeddings in lieu of ﬁne-tuning, akin to a continuous and differentiable\\ngeneralization of prompt engineering (Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al.,\\n2020; Liu et al., 2021). We include comparisons with Li & Liang (2021) in our experiment section.\\nHowever, this line of works can only scale up by using more special tokens in the prompt, which\\ntake up available sequence length for task tokens when positional embeddings are learned.\\nLow-Rank Structures in Deep Learning. Low-rank structure is very common in machine learn-\\ning. A lot of machine learning problems have certain intrinsic low-rank structure (Li et al., 2016;\\nCai et al., 2010; Li et al., 2018b; Grasedyck et al., 2013). Moreover, it is known that for many\\ndeep learning tasks, especially those with a heavily over-parametrized neural network, the learned\\nneural network will enjoy low-rank properties after training (Oymak et al., 2019). Some prior works\\neven explicitly impose the low-rank constraint when training the original neural network (Sainath\\net al., 2013; Povey et al., 2018; Zhang et al., 2014; Jaderberg et al., 2014; Zhao et al., 2016; Kho-\\ndak et al., 2021; Denil et al., 2014); however, to the best of our knowledge, none of these works\\nconsiders low-rank update to a frozen model for adaptation to downstream tasks . In theory liter-\\nature, it is known that neural networks outperform other classical learning methods, including the\\ncorresponding (ﬁnite-width) neural tangent kernels (Allen-Zhu et al., 2019; Li & Liang, 2018) when\\nthe underlying concept class has certain low-rank structure (Ghorbani et al., 2020; Allen-Zhu & Li,\\n2019; Allen-Zhu & Li, 2020a). Another theoretical result in Allen-Zhu & Li (2020b) suggests thatthe underlying concept class has certain low-rank structure (Ghorbani et al., 2020; Allen-Zhu & Li,\\n2019; Allen-Zhu & Li, 2020a). Another theoretical result in Allen-Zhu & Li (2020b) suggests that\\nlow-rank adaptations can be useful for adversarial training. In sum, we believe that our proposed\\nlow-rank adaptation update is well-motivated by the literature.\\n7 U NDERSTANDING THE LOW-RANK UPDATES\\nGiven the empirical advantage of LoRA, we hope to further explain the properties of the low-rank\\nadaptation learned from downstream tasks. Note that the low-rank structure not only lowers the\\nhardware barrier to entry which allows us to run multiple experiments in parallel, but also gives\\nbetter interpretability of how the update weights are correlated with the pre-trained weights. We\\nfocus our study on GPT-3 175B, where we achieved the largest reduction of trainable parameters\\n(up to 10,000\\x02) without adversely affecting task performances.\\nWe perform a sequence of empirical studies to answer the following questions: 1) Given a parameter\\nbudget constraint, which subset of weight matrices in a pre-trained Transformer should we adapt\\n9to maximize downstream performance? 2) Is the “optimal” adaptation matrix \\x01Wreally rank-\\ndeﬁcient ? If so, what is a good rank to use in practice? 3) What is the connection between \\x01Wand\\nW? Does \\x01Whighly correlate with W? How large is \\x01Wcomparing to W?\\nWe believe that our answers to question (2) and (3) shed light on the fundamental principles of using\\npre-trained language models for downstream tasks, which is a critical topic in NLP.\\n7.1 W HICH WEIGHT MATRICES IN TRANSFORMER SHOULD WEAPPLY LORA TO?\\nGiven a limited parameter budget, which types of weights should we adapt with LoRA to obtain\\nthe best performance on downstream tasks? As mentioned in Section 4.2, we only consider weight\\nmatrices in the self-attention module. We set a parameter budget of 18M (roughly 35MB if stored\\nin FP16) on GPT-3 175B, which corresponds to r= 8if we adapt one type of attention weights or\\nr= 4if we adapt two types, for all 96 layers. The result is presented in Table 5.\\n# of Trainable Parameters = 18M\\nWeight Type WqWkWvWoWq;WkWq;WvWq;Wk;Wv;Wo\\nRankr 8 8 8 8 4 4 2\\nWikiSQL (\\x060:5%) 70.4 70.0 73.0 73.2 71.4 73.7 73.7\\nMultiNLI (\\x060:1%) 91.0 90.8 91.0 91.3 91.3 91.3 91.7\\nTable 5: Validation accuracy on WikiSQL and MultiNLI after applying LoRA to different types of\\nattention weights in GPT-3, given the same number of trainable parameters. Adapting both Wqand\\nWvgives the best performance overall. We ﬁnd the standard deviation across random seeds to be\\nconsistent for a given dataset, which we report in the ﬁrst column.\\nNote that putting all the parameters in \\x01Wqor\\x01Wkresults in signiﬁcantly lower performance,\\nwhile adapting both WqandWvyields the best result. This suggests that even a rank of four\\ncaptures enough information in \\x01Wsuch that it is preferable to adapt more weight matrices than\\nadapting a single type of weights with a larger rank.\\n7.2 W HAT IS THE OPTIMAL RANKrFOR LORA?\\nWe turn our attention to the effect of rank ron model performance. We adapt fWq;Wvg,\\nfWq;Wk;Wv;Wcg, and justWqfor a comparison.\\nWeight Type r= 1r= 2r= 4r= 8r= 64\\nWikiSQL(\\x060:5%)Wq 68.8 69.6 70.5 70.4 70.0\\nWq;Wv 73.4 73.3 73.7 73.8 73.5\\nWq;Wk;Wv;Wo 74.1 73.7 74.0 74.0 73.9\\nMultiNLI (\\x060:1%)Wq 90.7 90.9 91.1 90.7 90.7\\nWq;Wv 91.3 91.4 91.3 91.6 91.4\\nWq;Wk;Wv;Wo 91.2 91.7 91.7 91.5 91.4\\nTable 6: Validation accuracy on WikiSQL and MultiNLI with different rank r. To our surprise, a\\nrank as small as one sufﬁces for adapting both WqandWvon these datasets while training Wqalone\\nneeds a larger r. We conduct a similar experiment on GPT-2 in Section H.2.\\nTable 6 shows that, surprisingly, LoRA already performs competitively with a very small r(more\\nso forfWq;Wvgthan justWq). This suggests the update matrix \\x01Wcould have a very small\\n“intrinsic rank”.6To further support this ﬁnding, we check the overlap of the subspaces learned by\\ndifferent choices of rand by different random seeds. We argue that increasing rdoes not cover a\\nmore meaningful subspace, which suggests that a low-rank adaptation matrix is sufﬁcient.\\n6However, we do not expect a small rto work for every task or dataset. Consider the following thought\\nexperiment: if the downstream task were in a different language than the one used for pre-training, retraining\\nthe entire model (similar to LoRA with r=dmodel ) could certainly outperform LoRA with a small r.\\n10Subspace similarity between different r.GivenAr=8andAr=64which are the learned adapta-\\ntion matrices with rank r= 8and64using the same pre-trained model , we perform singular value\\ndecomposition and obtain the right-singular unitary matrices UAr=8andUAr=64.7We hope to an-\\nswer: how much of the subspace spanned by the top isingular vectors in UAr=8(for1\\x14i\\x148) is\\ncontained in the subspace spanned by top jsingular vectors of UAr=64(for1\\x14j\\x1464)? We mea-\\nsure this quantity with a normalized subspace similarity based on the Grassmann distance (See Ap-\\npendix G for a more formal discussion)\\n\\x1e(Ar=8;Ar=64;i;j) =jjUi>\\nAr=8Uj\\nAr=64jj2\\nF\\nmin(i;j)2[0;1] (4)\\nwhereUi\\nAr=8represents the columns of UAr=8corresponding to the top- isingular vectors.\\n\\x1e(\\x01)has a range of [0;1], where 1represents a complete overlap of subspaces and 0a complete\\nseparation. See Figure 3 for how \\x1echanges as we vary iandj. We only look at the 48th layer\\n(out of 96) due to space constraint, but the conclusion holds for other layers as well, as shown\\nin Section H.1.\\n0.00.20.40.60.81.0\\n1\\n6\\n12\\n18\\n23\\n29\\n35\\n40\\n46\\n52\\n58\\nj12345678iWq\\n1\\n6\\n12\\n18\\n23\\n29\\n35\\n40\\n46\\n52\\n58\\njWv\\n12345678\\njWq\\n12345678\\njWv\\n(Ar=64,Ar=8,i,j)\\nFigure 3: Subspace similarity between column vectors of Ar=8andAr=64for both \\x01Wqand\\x01Wv.\\nThe third and the fourth ﬁgures zoom in on the lower-left triangle in the ﬁrst two ﬁgures. The top\\ndirections in r= 8are included in r= 64 , and vice versa.\\nWe make an important observation from Figure 3.\\nDirections corresponding to the top singular vector overlap signiﬁcantly between\\nAr=8andAr=64, while others do not. Speciﬁcally, \\x01Wv(resp. \\x01Wq) ofAr=8\\nand\\x01Wv(resp. \\x01Wq) ofAr=64share a subspace of dimension 1 with normalized\\nsimilarity>0:5, providing an explanation of why r= 1 performs quite well in our\\ndownstream tasks for GPT-3.\\nSince bothAr=8andAr=64are learned using the same pre-trained model, Figure 3 indicates that\\nthe top singular-vector directions of Ar=8andAr=64are the most useful, while other directions\\npotentially contain mostly random noises accumulated during training. Hence, the adaptation matrix\\ncan indeed have a very low rank.\\nSubspace similarity between different random seeds. We further conﬁrm this by plotting the\\nnormalized subspace similarity between two randomly seeded runs with r= 64 , shown in Figure 4.\\n\\x01Wqappears to have a higher “intrinsic rank” than \\x01Wv, since more common singular value direc-\\ntions are learned by both runs for \\x01Wq, which is in line with our empirical observation in Table 6.\\nAs a comparison, we also plot two random Gaussian matrices, which do not share any common\\nsingular value directions with each other.\\n7.3 H OWDOES THE ADAPTATION MATRIX \\x01WCOMPARE TO W?\\nWe further investigate the relationship between \\x01WandW. In particular, does \\x01Whighly correlate\\nwithW? (Or mathematically, is \\x01Wmostly contained in the top singular directions of W?) Also,\\n7Note that a similar analysis can be carried out with Band the left-singular unitary matrices – we stick with\\nAfor our experiments.\\n11'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tW_87FXOMpqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# システムを使わない場合"
      ],
      "metadata": {
        "id": "2pcOqQLLMptG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = summarizer(content, use_system_message=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZiRKMqnMnUw",
        "outputId": "91018107-41b3-46e3-993b-53bd5291322d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ABSTRACT\n",
            "- 大規模な事前学習と特定のタスクやドメインへの適応からなる自然言語処理の重要なパラダイム\n",
            "- GPT-3 175Bのような大規模モデルでは、全パラメータの再学習が困難になる\n",
            "- LoRAという手法を提案し、トランスフォーマーアーキテクチャの各層にトレーニング可能なランク分解行列を注入することで、トレーニング可能なパラメータの数を大幅に削減\n",
            "- LoRAは、GPT-3 175Bの場合、AdamでトレーニングされたGPT-3 175Bと比較して、トレーニング可能なパラメータの数を10,000倍、GPUメモリの要件を3倍に削減できる\n",
            "- LoRAは、RoBERTa、DeBERTa、GPT-2、GPT-3のモデル品質において、トレーニング可能なパラメータが少なく、トレーニングスループットが高く、アダプターとは異なり追加の推論レイテンシがないにもかかわらず、同等またはそれ以上の性能を発揮する\n",
            "- LoRAの効果についての実証的な調査を提供し、LoRAをPyTorchモデルと統合するためのパッケージと、RoBERTa、DeBERTa、GPT-2の実装とモデルチェックポイントを提供する\n",
            "\n",
            "2. INTRODUCTION\n",
            "- 自然言語処理の多くのアプリケーションでは、1つの大規模な事前学習言語モデルを複数のタスクに適応させることが求められる\n",
            "- ファインチューニングは、事前学習モデルのすべてのパラメータを再学習するため、大規模なモデルではコストが高くなる\n",
            "- LoRAは、トレーニング可能なパラメータの数を大幅に削減するための手法であり、トランスフォーマーアーキテクチャの各層にトレーニング可能なランク分解行列を注入する\n",
            "- LoRAは、トレーニング効率を向上させ、ハードウェアの制約を軽減する\n",
            "- LoRAは、アダプターやプレフィックスチューニングとは異なり、追加の推論レイテンシがない\n",
            "- LoRAは、他の手法と組み合わせることができる\n",
            "\n",
            "3. PROBLEM STATEMENT\n",
            "- ファインチューニングは、事前学習モデルのすべてのパラメータを再学習するため、大規模なモデルではコストが高くなる\n",
            "- LoRAは、トレーニング可能なパラメータの数を大幅に削減するための手法であり、トランスフォーマーアーキテクチャの各層にトレーニング可能なランク分解行列を注入する\n",
            "- ランクの低いアダプテーション行列を使用することで、ストレージと計算の効率を向上させる\n",
            "- LoRAは、ファインチューニングと比較して、トレーニングスループットが高く、トレーニング可能なパラメータが少なく、追加の推論レイテンシがないにもかかわらず、モデルの品質が同等またはそれ以上であることを示す\n",
            "- LoRAは、PyTorchモデルと統合するためのパッケージと、RoBERTa、DeBERTa、GPT-2の実装とモデルチェックポイントを提供する\n",
            "\n",
            "4. OUR METHOD\n",
            "- LoRAは、トレーニング可能なランク分解行列を既存の重み行列に注入することで、トレーニング可能なパラメータの数を削減する手法\n",
            "- LoRAは、トランスフォーマーアーキテクチャの特定の重み行列にのみ適用する\n",
            "- LoRAは、トレーニング可能なパラメータの数を削減することで、メモリとストレージの使用量を削減し、タスクの切り替えを効率化する\n",
            "- LoRAは、トレーニング効率を向上させ、ハードウェアの制約を軽減する\n",
            "- LoRAは、推論時に追加のレイテンシを導入せず、他の手法と組み合わせることができる\n",
            "\n",
            "5. EMPIRICAL EXPERIMENTS\n",
            "- RoBERTa、DeBERTa、GPT-2、GPT-3のモデルにLoRAを適用し、GLUEベンチマークや他のタスクでのパフォーマンスを評価する\n",
            "- LoRAは、他の手法と比較して優れたパフォーマンスを発揮することを示す\n",
            "- LoRAは、トレーニング可能なパラメータの数を削減し、トレーニング効率を向上させる\n",
            "- LoRAは、ファインチューニングと比較して追加の推論レイテンシがない\n",
            "- LoRAは、PyTorchモデルと統合するためのパッケージと、RoBERTa、DeBERTa、GPT-2の実装とモデルチェックポイントを提供する\n",
            "\n",
            "6. RELATED WORKS\n",
            "- トランスフォーマーモデルは、自然言語処理の分野で広く使用されている\n",
            "- ファインチューニングやプレフィックスチューニングなど、パラメータ効率の改善手法が提案されている\n",
            "- 低ランク構造は、機械学習の分野で一般的に使用されている\n",
            "- LoRAは、他の手法と比較して優れたパフォーマンスを発揮することを示す\n",
            "- LoRAは、他の手法と組み合わせることができる"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "Markdown(f'''\n",
        "{response.content}\n",
        "''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "id": "9lCdWxrYUPUX",
        "outputId": "1fef8f9c-48a4-49e6-ac33-1e1b6da1dbd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n1. ABSTRACT\n- 大規模な事前学習と特定のタスクやドメインへの適応からなる自然言語処理の重要なパラダイム\n- GPT-3 175Bのような大規模モデルでは、全パラメータの再学習が困難になる\n- LoRAという手法を提案し、トランスフォーマーアーキテクチャの各層にトレーニング可能なランク分解行列を注入することで、トレーニング可能なパラメータの数を大幅に削減\n- LoRAは、GPT-3 175Bの場合、AdamでトレーニングされたGPT-3 175Bと比較して、トレーニング可能なパラメータの数を10,000倍、GPUメモリの要件を3倍に削減できる\n- LoRAは、RoBERTa、DeBERTa、GPT-2、GPT-3のモデル品質において、トレーニング可能なパラメータが少なく、トレーニングスループットが高く、アダプターとは異なり追加の推論レイテンシがないにもかかわらず、同等またはそれ以上の性能を発揮する\n- LoRAの効果についての実証的な調査を提供し、LoRAをPyTorchモデルと統合するためのパッケージと、RoBERTa、DeBERTa、GPT-2の実装とモデルチェックポイントを提供する\n\n2. INTRODUCTION\n- 自然言語処理の多くのアプリケーションでは、1つの大規模な事前学習言語モデルを複数のタスクに適応させることが求められる\n- ファインチューニングは、事前学習モデルのすべてのパラメータを再学習するため、大規模なモデルではコストが高くなる\n- LoRAは、トレーニング可能なパラメータの数を大幅に削減するための手法であり、トランスフォーマーアーキテクチャの各層にトレーニング可能なランク分解行列を注入する\n- LoRAは、トレーニング効率を向上させ、ハードウェアの制約を軽減する\n- LoRAは、アダプターやプレフィックスチューニングとは異なり、追加の推論レイテンシがない\n- LoRAは、他の手法と組み合わせることができる\n\n3. PROBLEM STATEMENT\n- ファインチューニングは、事前学習モデルのすべてのパラメータを再学習するため、大規模なモデルではコストが高くなる\n- LoRAは、トレーニング可能なパラメータの数を大幅に削減するための手法であり、トランスフォーマーアーキテクチャの各層にトレーニング可能なランク分解行列を注入する\n- ランクの低いアダプテーション行列を使用することで、ストレージと計算の効率を向上させる\n- LoRAは、ファインチューニングと比較して、トレーニングスループットが高く、トレーニング可能なパラメータが少なく、追加の推論レイテンシがないにもかかわらず、モデルの品質が同等またはそれ以上であることを示す\n- LoRAは、PyTorchモデルと統合するためのパッケージと、RoBERTa、DeBERTa、GPT-2の実装とモデルチェックポイントを提供する\n\n4. OUR METHOD\n- LoRAは、トレーニング可能なランク分解行列を既存の重み行列に注入することで、トレーニング可能なパラメータの数を削減する手法\n- LoRAは、トランスフォーマーアーキテクチャの特定の重み行列にのみ適用する\n- LoRAは、トレーニング可能なパラメータの数を削減することで、メモリとストレージの使用量を削減し、タスクの切り替えを効率化する\n- LoRAは、トレーニング効率を向上させ、ハードウェアの制約を軽減する\n- LoRAは、推論時に追加のレイテンシを導入せず、他の手法と組み合わせることができる\n\n5. EMPIRICAL EXPERIMENTS\n- RoBERTa、DeBERTa、GPT-2、GPT-3のモデルにLoRAを適用し、GLUEベンチマークや他のタスクでのパフォーマンスを評価する\n- LoRAは、他の手法と比較して優れたパフォーマンスを発揮することを示す\n- LoRAは、トレーニング可能なパラメータの数を削減し、トレーニング効率を向上させる\n- LoRAは、ファインチューニングと比較して追加の推論レイテンシがない\n- LoRAは、PyTorchモデルと統合するためのパッケージと、RoBERTa、DeBERTa、GPT-2の実装とモデルチェックポイントを提供する\n\n6. RELATED WORKS\n- トランスフォーマーモデルは、自然言語処理の分野で広く使用されている\n- ファインチューニングやプレフィックスチューニングなど、パラメータ効率の改善手法が提案されている\n- 低ランク構造は、機械学習の分野で一般的に使用されている\n- LoRAは、他の手法と比較して優れたパフォーマンスを発揮することを示す\n- LoRAは、他の手法と組み合わせることができる\n"
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_token(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miirURajMnYB",
        "outputId": "bfbf29a8-c2da-4082-f511-8ee2b287f67c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total token: 1812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lpWCyyjhM1jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IfvNOYPJM1or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# システムを使う場合"
      ],
      "metadata": {
        "id": "woCuP9KTM1rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = summarizer(content, use_system_message=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqioJTfFMnbF",
        "outputId": "e7a55636-19cd-4b40-ca87-b8ffd205028a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ABSTRACT\n",
            "- 論文の要点: LoRAは、大規模な言語モデルを特定のタスクやドメインに適応させるための低ランク適応手法です。LoRAは、事前学習済みモデルの重みを凍結し、各層にトレーニング可能なランク分解行列を注入することで、トレーニング可能なパラメータの数を大幅に削減します。LoRAは、GPT-3 175Bと比較して、トレーニング可能なパラメータの数を10,000倍、GPUメモリの要件を3倍に削減することができます。また、LoRAは、RoBERTa、DeBERTa、GPT-2、GPT-3のモデル品質において、トレーニング可能なパラメータが少なく、トレーニングスループットが高く、アダプターとは異なり追加の推論レイテンシがないにもかかわらず、同等以上のパフォーマンスを発揮します。\n",
            "\n",
            "2. INTRODUCTION\n",
            "- 論文の背景: 自然言語処理の重要なパラダイムは、一般的なドメインのデータでの大規模な事前学習と、特定のタスクやドメインへの適応です。しかし、より大きなモデルを事前学習するにつれて、全パラメータを再学習する完全なファインチューニングは実現困難になります。そこで、LoRAでは、事前学習済みモデルの重みを凍結し、各層にトレーニング可能なランク分解行列を注入することで、トレーニング可能なパラメータの数を大幅に削減します。\n",
            "\n",
            "3. PROBLEM STATEMENT\n",
            "- 問題の概要: ファインチューニングは、事前学習済みモデルのすべてのパラメータを更新するため、パラメータ数が元のモデルと同じだけ必要になります。しかし、大規模なモデルでは、ファインチューニングはモデルのデプロイメントにおける重要な課題となります。LoRAは、ファインチューニングと比較してトレーニング可能なパラメータの数を大幅に削減し、モデルの品質を維持することができます。\n",
            "\n",
            "4. OUR METHOD\n",
            "- メソッドの概要: LoRAは、事前学習済みモデルの重みにトレーニング可能なランク分解行列を注入することで、トレーニング可能なパラメータの数を削減します。LoRAは、モデルのトレーニング効率を向上させ、ハードウェアの制約を緩和します。また、LoRAは、ファインチューニングと比較して推論レイテンシを追加しないため、デプロイメント時の効率も向上させます。\n",
            "\n",
            "5. EMPIRICAL EXPERIMENTS\n",
            "- 実験結果の概要: LoRAは、RoBERTa、DeBERTa、GPT-2、GPT-3のモデルにおいて、トレーニング可能なパラメータの数を削減しながら、モデルの品質を維持することができます。また、GPT-3 175Bにおいても、LoRAはファインチューニングと比較して優れたパフォーマンスを発揮します。\n",
            "\n",
            "6. RELATED WORKS\n",
            "- 関連研究の概要: LoRAは、他のパラメータ効率の高い適応手法と比較しても優れたパフォーマンスを発揮します。また、LoRAは、事前学習済み言語モデルを下流のタスクに適応させるための基本原則を明らかにするものです。"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(f'''\n",
        "{response.content}\n",
        "''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "ls0ZMPuRMneI",
        "outputId": "e3c84e24-16f4-48f4-ffcf-c97ef2376608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n1. ABSTRACT\n- 論文の要点: LoRAは、大規模な言語モデルを特定のタスクやドメインに適応させるための低ランク適応手法です。LoRAは、事前学習済みモデルの重みを凍結し、各層にトレーニング可能なランク分解行列を注入することで、トレーニング可能なパラメータの数を大幅に削減します。LoRAは、GPT-3 175Bと比較して、トレーニング可能なパラメータの数を10,000倍、GPUメモリの要件を3倍に削減することができます。また、LoRAは、RoBERTa、DeBERTa、GPT-2、GPT-3のモデル品質において、トレーニング可能なパラメータが少なく、トレーニングスループットが高く、アダプターとは異なり追加の推論レイテンシがないにもかかわらず、同等以上のパフォーマンスを発揮します。\n\n2. INTRODUCTION\n- 論文の背景: 自然言語処理の重要なパラダイムは、一般的なドメインのデータでの大規模な事前学習と、特定のタスクやドメインへの適応です。しかし、より大きなモデルを事前学習するにつれて、全パラメータを再学習する完全なファインチューニングは実現困難になります。そこで、LoRAでは、事前学習済みモデルの重みを凍結し、各層にトレーニング可能なランク分解行列を注入することで、トレーニング可能なパラメータの数を大幅に削減します。\n\n3. PROBLEM STATEMENT\n- 問題の概要: ファインチューニングは、事前学習済みモデルのすべてのパラメータを更新するため、パラメータ数が元のモデルと同じだけ必要になります。しかし、大規模なモデルでは、ファインチューニングはモデルのデプロイメントにおける重要な課題となります。LoRAは、ファインチューニングと比較してトレーニング可能なパラメータの数を大幅に削減し、モデルの品質を維持することができます。\n\n4. OUR METHOD\n- メソッドの概要: LoRAは、事前学習済みモデルの重みにトレーニング可能なランク分解行列を注入することで、トレーニング可能なパラメータの数を削減します。LoRAは、モデルのトレーニング効率を向上させ、ハードウェアの制約を緩和します。また、LoRAは、ファインチューニングと比較して推論レイテンシを追加しないため、デプロイメント時の効率も向上させます。\n\n5. EMPIRICAL EXPERIMENTS\n- 実験結果の概要: LoRAは、RoBERTa、DeBERTa、GPT-2、GPT-3のモデルにおいて、トレーニング可能なパラメータの数を削減しながら、モデルの品質を維持することができます。また、GPT-3 175Bにおいても、LoRAはファインチューニングと比較して優れたパフォーマンスを発揮します。\n\n6. RELATED WORKS\n- 関連研究の概要: LoRAは、他のパラメータ効率の高い適応手法と比較しても優れたパフォーマンスを発揮します。また、LoRAは、事前学習済み言語モデルを下流のタスクに適応させるための基本原則を明らかにするものです。\n"
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uBmi_-wOVQNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "human_template=\"\"\"\n",
        "次の論文から下記の質問に答えよ\n",
        "論文:{thesis}\n",
        "\n",
        "質問:{question}\n",
        "\"\"\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\n",
        "prompt = chat_prompt.format_prompt(thesis=content, question=\"どのような評価を行って、モデルの品質が維持できることを確かめたのですか？\").to_messages()\n",
        "response = llm(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQEGSoX5VLjT",
        "outputId": "446034d2-893b-451e-d644-a064bbc56fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "論文では、LoRAの性能を評価するために、RoBERTa、DeBERTa、GPT-2、およびGPT-3のさまざまなタスクでの実験を行いました。具体的には、GLUEベンチマーク、E2E NLG Challenge、WikiSQL、MultiNLI、SAMSumなどのタスクでの性能を評価しました。これらの実験により、LoRAがフルファインチューニングと同等またはそれ以上の性能を提供することが示されました。また、LoRAがパラメータ数を大幅に削減できることも確認されました。"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(f'''\n",
        "{response.content}\n",
        "''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "QnX49K5FMngg",
        "outputId": "0b1f2ccb-4024-4a93-f0ab-b8bf49bf2d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n論文では、LoRAの性能を評価するために、RoBERTa、DeBERTa、GPT-2、およびGPT-3のさまざまなタスクでの実験を行いました。具体的には、GLUEベンチマーク、E2E NLG Challenge、WikiSQL、MultiNLI、SAMSumなどのタスクでの性能を評価しました。これらの実験により、LoRAがフルファインチューニングと同等またはそれ以上の性能を提供することが示されました。また、LoRAがパラメータ数を大幅に削減できることも確認されました。\n"
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5phbBn-5M55w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zkF7F0zhM6ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-X6VInPlM6zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ar_e8HcYM59t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"less_is_more.pdf\")\n",
        "content = preprocess_text(loader)"
      ],
      "metadata": {
        "id": "fKVotZe3M6Ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1PZJTb59M8Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# システムを使わない場合\n",
        "response = summarizer(content, use_system_message=False)"
      ],
      "metadata": {
        "id": "gbkz7BDQM8T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# システムを使う場合\n",
        "response = summarizer(content, use_system_message=True)"
      ],
      "metadata": {
        "id": "ZSATJrI3M-OT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r9ACjYtCNCz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3fXeS_t4NC3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dWS9fx4ENC6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 質問もできる"
      ],
      "metadata": {
        "id": "mV9TJDOENDot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "human_template=\"\"\"\n",
        "次の論文から下記の質問に答えよ\n",
        "論文:{thesis}\n",
        "\n",
        "質問:{question}\n",
        "\"\"\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\n",
        "prompt = chat_prompt.format_prompt(thesis=content, question=\"約1,000のプロンプトとは具体的にどういうものですか？\").to_messages()\n",
        "response = llm(prompt)"
      ],
      "metadata": {
        "id": "DmirIZgyNC8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "24qUnUxAM-SR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sx3rIIQaM-Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FWP479aiM-YY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}